{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'learning_rate': 5e-5,\n",
    "    'foggy2data_ratio': 1.0,\n",
    "    'foggy2data_interpolate': 'nearest',\n",
    "    'foggy2data_use_round': False,\n",
    "    'logit2scaled_logit_interpolate': 'bilinear',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RefineNet_ResNet(Resnet101)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.refinenet_resnet import refinenet_resnet101\n",
    "\n",
    "pretrained_weight_dir = '../../refinenet-pytorch/pretrained/Cityscapes'\n",
    "model = refinenet_resnet101(pretrained_weight_dir).eval()\n",
    "model_name = '{}({})'.format(str(model).split('(')[0], str(model.resnet).split('(')[0])\n",
    "print(model_name)\n",
    "\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.refinenet_resnet import RefineNet_ResNet\n",
    "\n",
    "class ModelParallel_RefineNet_ResNet(RefineNet_ResNet):\n",
    "    def __init__(self, dev0, dev1, *args, **kwds):\n",
    "        self.dev0 = dev0\n",
    "        self.dev1 = dev1\n",
    "        super(ModelParallel_RefineNet_ResNet, self).__init__(*args, **kwds)\n",
    "        self.resnet = self.resnet.to(dev0)\n",
    "        self.refinenets = self.refinenets.to(dev1)\n",
    "        self.clf = self.clf.to(dev0)\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x.to(self.dev0))\n",
    "        x = self.refinenets(tuple([x_.to(self.dev1) for x_ in x]))\n",
    "        x = self.clf(x.to(self.dev0))\n",
    "        return x\n",
    "\n",
    "model = ModelParallel_RefineNet_ResNet('cuda:0', 'cuda:1', model.resnet, model.refinenets, model.clf)\n",
    "\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvCityscapes\n",
      "Dataset EnvCityscapes\n",
      "    Number of datapoints: 498\n",
      "    Root location: /workbench/data/Cityscapes\n",
      "    Env: ['', 'foggyDBF', 'transmittanceDBF']\t[[''], ['beta_0.01'], ['beta_0.01']]\n",
      "    Split: train\n",
      "    Mode: gtFine\n",
      "    Type: ['semantic', 'color']\n",
      "    StandardTransform\n",
      "Transform: Lambda()\n",
      "Target transform: Lambda()\n",
      "Dataset EnvCityscapes\n",
      "    Number of datapoints: 52\n",
      "    Root location: /workbench/data/Cityscapes\n",
      "    Env: ['', 'foggyDBF', 'transmittanceDBF']\t[[''], ['beta_0.01'], ['beta_0.01']]\n",
      "    Split: val\n",
      "    Mode: gtFine\n",
      "    Type: ['semantic', 'color']\n",
      "    StandardTransform\n",
      "Transform: Lambda()\n",
      "Target transform: Lambda()\n",
      "Dataset Zurich\n",
      "    Number of datapoints: 40\n",
      "    Root location: /workbench/data/Foggy_Zurich\n",
      "    Environment: foggy\n",
      "    Split: testv2\n",
      "    Type: ['gt_labelIds', 'RGB']\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Lambda()\n",
      "           )\n",
      "Target transform: Lambda()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import datasets\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda img: img*255)])\n",
    "target_transform = transforms.Lambda(lambda img: torch.from_numpy(np.array(img)))\n",
    "\n",
    "foggy_cityscapes = datasets.EnvCityscapes(\n",
    "    '/workbench/data/Cityscapes', split='train',\n",
    "    env=[\"\", 'foggyDBF', 'transmittanceDBF'],\n",
    "    env_type=[[''], ['beta_0.01'], ['beta_0.01']],\n",
    "    target_type=['semantic', 'color'],\n",
    "    transform=transforms.Lambda(lambda imgs: tuple(map(transform, imgs))),\n",
    "    target_transform=transforms.Lambda(lambda imgs: tuple(map(target_transform, imgs))),\n",
    ")\n",
    "foggy_cityscapes_val = datasets.EnvCityscapes(\n",
    "    '/workbench/data/Cityscapes', split='val',\n",
    "    env=[\"\", 'foggyDBF', 'transmittanceDBF'],\n",
    "    env_type=[[''], ['beta_0.01'], ['beta_0.01']],\n",
    "    target_type=['semantic', 'color'],\n",
    "    transform=transforms.Lambda(lambda imgs: tuple(map(transform, imgs))),\n",
    "    target_transform=transforms.Lambda(lambda imgs: tuple(map(target_transform, imgs))),\n",
    ")\n",
    "zurich_testv2 = datasets.Zurich(\n",
    "    '/workbench/data/Foggy_Zurich', split='testv2',\n",
    "    target_type=['gt_labelIds', 'RGB'],\n",
    "    transform=transform,\n",
    "    target_transform=transforms.Lambda(lambda imgs: tuple(map(target_transform, imgs))),\n",
    ")\n",
    "\n",
    "with open('/workbench/data/Cityscapes/foggy_trainval_refined_filenames.txt') as f:\n",
    "    trainval_refined_filenames = sorted([line.strip() for line in f.readlines()])\n",
    "\n",
    "images, targets = map(np.array, [foggy_cityscapes.images, foggy_cityscapes.targets])\n",
    "refine_mask = np.zeros(images.shape[0]).astype(np.bool)\n",
    "for i, image in enumerate(images):\n",
    "    for filename in trainval_refined_filenames:\n",
    "        if filename in image[0]:\n",
    "            refine_mask[i] = True\n",
    "            break\n",
    "foggy_cityscapes.images, foggy_cityscapes.targets = images[refine_mask], targets[refine_mask]\n",
    "\n",
    "images, targets = map(np.array, [foggy_cityscapes_val.images, foggy_cityscapes_val.targets])\n",
    "refine_mask = np.zeros(images.shape[0]).astype(np.bool)\n",
    "for i, image in enumerate(images):\n",
    "    for filename in trainval_refined_filenames:\n",
    "        if filename in image[0]:\n",
    "            refine_mask[i] = True\n",
    "foggy_cityscapes_val.images, foggy_cityscapes_val.targets = images[refine_mask], targets[refine_mask]\n",
    "\n",
    "dataset_name = str(foggy_cityscapes).split()[1]\n",
    "print(dataset_name)\n",
    "print(foggy_cityscapes)\n",
    "print(foggy_cityscapes_val)\n",
    "print(zurich_testv2)\n",
    "\n",
    "train_loader = DataLoader(foggy_cityscapes, shuffle=True)\n",
    "valid_loader = DataLoader(foggy_cityscapes_val)\n",
    "test_loader = DataLoader(zurich_testv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import datasets\n",
    "\n",
    "# classes = datasets.Cityscapes.classes\n",
    "# id2label = {cls.id:cls for cls in classes}\n",
    "# train_id2label = {cls.train_id:cls for cls in classes}\n",
    "# pred_argmax2label = {cls.train_id:cls for cls in classes}\n",
    "# pred_argmax2label[19] = pred_argmax2label[255]\n",
    "# pred_argmax2label.pop(255);\n",
    "\n",
    "# def foggy2data(foggy):\n",
    "#     data = F.interpolate(foggy, None, 0.8*1.2, mode='bilinear')-128\n",
    "#     return data\n",
    "# def labelId2gt(labelId):\n",
    "#     gt = np.vectorize(lambda x: id2label[x].train_id)(labelId.numpy())\n",
    "#     gt = np.vectorize(lambda x: 19 if x == 255 else x)(gt)\n",
    "#     gt = torch.from_numpy(gt)\n",
    "#     return gt\n",
    "# def logit2scaled_logit(logit, shape):\n",
    "#     scaled_logit = F.interpolate(logit, shape, mode='bilinear')\n",
    "#     return scaled_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import datasets\n",
    "\n",
    "classes = datasets.Cityscapes.classes\n",
    "id2label = {cls.id:cls for cls in classes}\n",
    "train_id2label = {cls.train_id:cls for cls in classes}\n",
    "pred_argmax2label = {cls.train_id:cls for cls in classes}\n",
    "pred_argmax2label[19] = pred_argmax2label[255]\n",
    "pred_argmax2label.pop(255);\n",
    "\n",
    "def foggy2data(foggy):\n",
    "    if config['foggy2data_ratio'] != 1.0:\n",
    "        data = F.interpolate(foggy, None, config['foggy2data_ratio'], mode=config['foggy2data_interpolate'])\n",
    "        if config['foggy2data_use_round']:\n",
    "            data = data.round()\n",
    "    else:\n",
    "        data = foggy\n",
    "    return data-128\n",
    "def labelId2gt(labelId):\n",
    "    gt = np.vectorize(lambda x: id2label[x].train_id)(labelId.numpy())\n",
    "    gt = np.vectorize(lambda x: 19 if x == 255 else x)(gt)\n",
    "    gt = torch.from_numpy(gt)\n",
    "    return gt\n",
    "def logit2scaled_logit(logit, shape):\n",
    "    scaled_logit = F.interpolate(logit, shape, mode=config['logit2scaled_logit_interpolate'])\n",
    "    return scaled_logit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/yonyeoseok/refinenet-pytorch\" target=\"_blank\">https://app.wandb.ai/yonyeoseok/refinenet-pytorch</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/yonyeoseok/refinenet-pytorch/runs/161gyu2y\" target=\"_blank\">https://app.wandb.ai/yonyeoseok/refinenet-pytorch/runs/161gyu2y</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.8.33 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(project='refinenet-pytorch', name=model_name, tags=dataset_name, config=config)\n",
    "\n",
    "def wandb_scores(scores, split, step, commit):\n",
    "    for k, v in scores[0].items():\n",
    "        wandb.log({'{}_{}'.format(split, k): v}, step=step, commit=commit)\n",
    "    wandb.run.summary['{}_IoU'.format(split).format(split, k)] = list(scores[1].values())\n",
    "\n",
    "def train_model(model, optimizer, criteria, data, gt):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logit = model(data).to(gt.device)\n",
    "    scaled_logit = logit2scaled_logit(logit, gt.shape[-2:])\n",
    "    loss = criteria(scaled_logit, gt)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def eval_model(model, running_metrics, data, gt):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logit = model(data).to(gt.device)\n",
    "        scaled_logit = logit2scaled_logit(logit, gt.shape[-2:])\n",
    "        loss = criteria(scaled_logit, gt)\n",
    "        \n",
    "        running_metrics.update(gt.numpy(), scaled_logit.argmax(1).numpy())\n",
    "    return loss\n",
    "\n",
    "def eval_model_valid_loader(model, valid_loader, step, commit):\n",
    "    epoch_loss = 0\n",
    "    running_metrics = runningScore(20)\n",
    "    for i, ((clear, foggy, trans), (labelId, color)) in enumerate(tqdm(valid_loader)):\n",
    "        loss = eval_model(model, running_metrics,\n",
    "                          foggy2data(foggy),\n",
    "                          labelId2gt(labelId))\n",
    "        epoch_loss += loss\n",
    "        \n",
    "    scores = running_metrics.get_scores()\n",
    "    \n",
    "    wandb_scores(scores, split='Valid', step=step, commit=False)\n",
    "    wandb.log({'Valid_Epoch_Loss': epoch_loss / len(valid_loader)}, step=step, commit=commit)\n",
    "\n",
    "    return epoch_loss / len(valid_loader), scores\n",
    "\n",
    "def eval_model_test_loader(model, test_loader, step, commit):\n",
    "    epoch_loss = 0\n",
    "    running_metrics = runningScore(20)\n",
    "    for i, (foggy, (labelId, color)) in enumerate(tqdm(test_loader)):\n",
    "        loss = eval_model(model, running_metrics,\n",
    "                          foggy2data(foggy),\n",
    "                          labelId2gt(labelId))\n",
    "        epoch_loss += loss\n",
    "        \n",
    "    scores = running_metrics.get_scores()\n",
    "    \n",
    "    wandb_scores(scores, split='Test', step=step, commit=False)\n",
    "    wandb.log({'Test_Epoch_Loss': epoch_loss / len(test_loader)}, step=step, commit=commit)\n",
    "    \n",
    "    return epoch_loss / len(test_loader), scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]/opt/conda/conda-bld/pytorch_1587428207430/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Tensor input, Number alpha, Tensor other, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor input, Tensor other, *, Number alpha, Tensor out)\n",
      "/opt/conda/envs/py37torch1.5/lib/python3.8/site-packages/torch/nn/functional.py:2970: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n",
      "100%|██████████| 52/52 [01:42<00:00,  1.97s/it]\n",
      "../utils/metrics.py:32: RuntimeWarning: invalid value encountered in true_divide\n",
      "  acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
      "../utils/metrics.py:34: RuntimeWarning: invalid value encountered in true_divide\n",
      "  iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.8.33 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]/opt/conda/envs/py37torch1.5/lib/python3.8/site-packages/torch/nn/functional.py:2970: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n",
      "100%|██████████| 40/40 [01:12<00:00,  1.81s/it]\n",
      "../utils/metrics.py:32: RuntimeWarning: invalid value encountered in true_divide\n",
      "  acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
      "100%|██████████| 498/498 [22:52<00:00,  2.76s/it]\n",
      "100%|██████████| 52/52 [01:44<00:00,  2.01s/it]\n",
      "100%|██████████| 40/40 [01:11<00:00,  1.80s/it]\n",
      "100%|██████████| 498/498 [22:22<00:00,  2.70s/it]\n",
      "100%|██████████| 52/52 [01:49<00:00,  2.10s/it]\n",
      "100%|██████████| 40/40 [01:14<00:00,  1.86s/it]\n",
      "100%|██████████| 498/498 [22:32<00:00,  2.72s/it]\n",
      "100%|██████████| 52/52 [01:40<00:00,  1.93s/it]\n",
      "100%|██████████| 40/40 [01:14<00:00,  1.87s/it]\n",
      "100%|██████████| 498/498 [22:09<00:00,  2.67s/it]\n",
      "100%|██████████| 52/52 [01:48<00:00,  2.09s/it]\n",
      "100%|██████████| 40/40 [01:10<00:00,  1.75s/it]\n",
      "100%|██████████| 498/498 [22:32<00:00,  2.72s/it]\n",
      "100%|██████████| 52/52 [01:46<00:00,  2.06s/it]\n",
      "100%|██████████| 40/40 [01:16<00:00,  1.92s/it]\n",
      "100%|██████████| 498/498 [22:24<00:00,  2.70s/it]\n",
      "100%|██████████| 52/52 [01:49<00:00,  2.10s/it]\n",
      "100%|██████████| 40/40 [01:09<00:00,  1.75s/it]\n",
      "100%|██████████| 498/498 [22:33<00:00,  2.72s/it]\n",
      "100%|██████████| 52/52 [01:51<00:00,  2.14s/it]\n",
      "100%|██████████| 40/40 [01:17<00:00,  1.94s/it]\n",
      "100%|██████████| 498/498 [23:05<00:00,  2.78s/it]\n",
      "100%|██████████| 52/52 [01:52<00:00,  2.16s/it]\n",
      "100%|██████████| 40/40 [01:13<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.metrics import runningScore\n",
    "\n",
    "epoch_loss, scores = eval_model_valid_loader(model, valid_loader, step=0, commit=False)\n",
    "epoch_loss, scores = eval_model_test_loader(model, test_loader, step=0, commit=True)\n",
    "\n",
    "for epoch in range(1, 9):\n",
    "    epoch_loss = 0\n",
    "    for i, ((clear, foggy, trans), (labelId, color)) in enumerate(tqdm(train_loader)):\n",
    "        loss = train_model(model, optimizer, criteria,\n",
    "                           foggy2data(foggy),\n",
    "                           labelId2gt(labelId))\n",
    "        epoch_loss += loss\n",
    "        wandb.log({'Train_Batch_Loss': loss}, step=(i+1) + len(train_loader)*(epoch-1))\n",
    "    torch.save(model.state_dict(), os.path.join(wandb.run.dir, 'state_dict.{:02d}.pth').format(epoch))\n",
    "\n",
    "    epoch_loss, scores = eval_model_valid_loader(model, valid_loader, step=len(train_loader)*epoch, commit=False)\n",
    "    epoch_loss, scores = eval_model_test_loader(model, test_loader, step=len(train_loader)*epoch, commit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37torch1.5",
   "language": "python",
   "name": "py37torch1.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
