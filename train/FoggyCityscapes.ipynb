{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RefineNet_ResNet(Resnet101)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models.refinenet_resnet import refinenet_resnet101\n",
    "\n",
    "pretrained_weight_dir = '../../refinenet-pytorch/pretrained/Cityscapes'\n",
    "model = refinenet_resnet101(pretrained_weight_dir).eval()\n",
    "model_name = '{}({})'.format(str(model).split('(')[0], str(model.resnet).split('(')[0])\n",
    "print(model_name)\n",
    "\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-5)\n",
    "\n",
    "model = nn.DistributedDataParallel(model.cuda(), device_ids=[0, 1]).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvCityscapes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import datasets\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda img: img*255)])\n",
    "target_transform = transforms.Lambda(lambda img: torch.from_numpy(np.array(img)))\n",
    "\n",
    "foggy_cityscapes = datasets.EnvCityscapes(\n",
    "    '/workbench/data/Cityscapes', split='train',\n",
    "    env=[\"\", 'foggyDBF', 'transmittanceDBF'],\n",
    "    env_type=[[''], ['beta_0.01'], ['beta_0.01']],\n",
    "    target_type=['semantic', 'color'],\n",
    "    transform=transforms.Lambda(lambda imgs: tuple(map(transform, imgs))),\n",
    "    target_transform=transforms.Lambda(lambda imgs: tuple(map(target_transform, imgs))),\n",
    ")\n",
    "foggy_cityscapes_val = datasets.EnvCityscapes(\n",
    "    '/workbench/data/Cityscapes', split='val',\n",
    "    env=[\"\", 'foggyDBF', 'transmittanceDBF'],\n",
    "    env_type=[[''], ['beta_0.01'], ['beta_0.01']],\n",
    "    target_type=['semantic', 'color'],\n",
    "    transform=transforms.Lambda(lambda imgs: tuple(map(transform, imgs))),\n",
    "    target_transform=transforms.Lambda(lambda imgs: tuple(map(target_transform, imgs))),\n",
    ")\n",
    "zurich_testv2 = datasets.Zurich(\n",
    "    '/workbench/data/Foggy_Zurich', split='testv2',\n",
    "    target_type=['gt_labelIds', 'RGB'],\n",
    "    transform=transform,\n",
    "    target_transform=transforms.Lambda(lambda imgs: tuple(map(target_transform, imgs))),\n",
    ")\n",
    "\n",
    "with open('/workbench/data/Cityscapes/foggy_trainval_refined_filenames.txt') as f:\n",
    "    trainval_refined_filenames = sorted([line.strip() for line in f.readlines()])\n",
    "\n",
    "images, targets = map(np.array, [foggy_cityscapes.images, foggy_cityscapes.targets])\n",
    "refine_mask = np.zeros(images.shape[0]).astype(np.bool)\n",
    "for i, image in enumerate(images):\n",
    "    for filename in trainval_refined_filenames:\n",
    "        if filename in image[0]:\n",
    "            refine_mask[i] = True\n",
    "            break\n",
    "foggy_cityscapes.images, foggy_cityscapes.targets = images[refine_mask], targets[refine_mask]\n",
    "\n",
    "images, targets = map(np.array, [foggy_cityscapes_val.images, foggy_cityscapes_val.targets])\n",
    "refine_mask = np.zeros(images.shape[0]).astype(np.bool)\n",
    "for i, image in enumerate(images):\n",
    "    for filename in trainval_refined_filenames:\n",
    "        if filename in image[0]:\n",
    "            refine_mask[i] = True\n",
    "foggy_cityscapes_val.images, foggy_cityscapes_val.targets = images[refine_mask], targets[refine_mask]\n",
    "\n",
    "dataset_name = str(foggy_cityscapes).split()[1]\n",
    "print(dataset_name)\n",
    "\n",
    "train_loader = DataLoader(foggy_cityscapes)\n",
    "valid_loader = DataLoader(foggy_cityscapes_val)\n",
    "test_loader = DataLoader(zurich_testv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import datasets\n",
    "\n",
    "classes = datasets.Cityscapes.classes\n",
    "id2label = {cls.id:cls for cls in classes}\n",
    "train_id2label = {cls.train_id:cls for cls in classes}\n",
    "pred_argmax2label = {cls.train_id:cls for cls in classes}\n",
    "pred_argmax2label[19] = pred_argmax2label[255]\n",
    "pred_argmax2label.pop(255);\n",
    "\n",
    "def foggy2data(foggy):\n",
    "    data = F.interpolate(foggy, None, 0.7, mode='bilinear')-128\n",
    "    return data\n",
    "def labelId2gt(labelId):\n",
    "    gt = np.vectorize(lambda x: id2label[x].train_id)(labelId.numpy())\n",
    "    gt = np.vectorize(lambda x: 19 if x == 255 else x)(gt)\n",
    "    gt = torch.from_numpy(gt)\n",
    "    return gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/yonyeoseok/refinenet-pytorch\" target=\"_blank\">https://app.wandb.ai/yonyeoseok/refinenet-pytorch</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/yonyeoseok/refinenet-pytorch/runs/108n8mgj\" target=\"_blank\">https://app.wandb.ai/yonyeoseok/refinenet-pytorch/runs/108n8mgj</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "W&B Run: https://app.wandb.ai/yonyeoseok/refinenet-pytorch/runs/108n8mgj"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(project='refinenet-pytorch', name=model_name, tags=dataset_name)\n",
    "\n",
    "def wandb_scores(scores, split):\n",
    "    for k, v in scores[0].items():\n",
    "        wandb.log({'Valid_{}'.format(k): v}, commit=False)\n",
    "    wandb.run.summary['{}_IoU'.format(split).format(split, k)] = list(scores[1].values())\n",
    "\n",
    "def train_model(model, optimizer, criteria, data, gt):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logit = model(data).to(gt.device)\n",
    "    scaled_logit = F.interpolate(logit, gt.shape[-2:], mode='bilinear')\n",
    "    loss = criteria(scaled_logit, gt)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def eval_model(model, running_metrics, data, gt):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logit = model(data).to(gt.device)\n",
    "        scaled_logit = F.interpolate(logit, gt.shape[-2:], mode='bilinear')\n",
    "        loss = criteria(scaled_logit, gt)\n",
    "        \n",
    "        running_metrics.update(gt.numpy(), scaled_logit.argmax(1).numpy())\n",
    "    return loss\n",
    "\n",
    "def eval_model_valid_loader(model, valid_loader, step, commit):\n",
    "    epoch_loss = 0\n",
    "    running_metrics = runningScore(20)\n",
    "    for i, ((clear, foggy, trans), (labelId, color)) in enumerate(tqdm(valid_loader)):\n",
    "        loss = eval_model(model, running_metrics,\n",
    "                          foggy2data(foggy),\n",
    "                          labelId2gt(labelId))\n",
    "        epoch_loss += loss\n",
    "        \n",
    "    scores = running_metrics.get_scores()\n",
    "    \n",
    "    wandb_scores(scores, 'Valid')\n",
    "    wandb.log({'Valid_Epoch_Loss': epoch_loss / len(valid_loader)}, step=step, commit=commit)\n",
    "\n",
    "    return epoch_loss / len(valid_loader), scores\n",
    "\n",
    "def eval_model_test_loader(model, test_loader, step, commit):\n",
    "    epoch_loss = 0\n",
    "    running_metrics = runningScore(20)\n",
    "    for i, (foggy, (labelId, color)) in enumerate(tqdm(test_loader)):\n",
    "        loss = eval_model(model, running_metrics,\n",
    "                          foggy2data(foggy),\n",
    "                          labelId2gt(labelId))\n",
    "        epoch_loss += loss\n",
    "        \n",
    "    scores = running_metrics.get_scores()\n",
    "    \n",
    "    wandb_scores(scores, 'Test')\n",
    "    wandb.log({'Test_Epoch_Loss': epoch_loss / len(test_loader)}, step=step, commit=commit)\n",
    "    \n",
    "    return epoch_loss / len(test_loader), scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]/opt/conda/envs/py37torch1.5/lib/python3.8/site-packages/torch/nn/functional.py:2970: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n",
      "/opt/conda/envs/py37torch1.5/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/opt/conda/conda-bld/pytorch_1587428207430/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Tensor input, Number alpha, Tensor other, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor input, Tensor other, *, Number alpha, Tensor out)\n",
      "100%|██████████| 52/52 [01:39<00:00,  1.90s/it]\n",
      "../utils/metrics.py:32: RuntimeWarning: invalid value encountered in true_divide\n",
      "  acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
      "../utils/metrics.py:34: RuntimeWarning: invalid value encountered in true_divide\n",
      "  iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
      "100%|██████████| 40/40 [01:08<00:00,  1.70s/it]\n",
      "  0%|          | 1/498 [00:03<30:36,  3.70s/it]/opt/conda/envs/py37torch1.5/lib/python3.8/site-packages/torch/nn/functional.py:2970: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\"Default upsampling behavior when mode={} is changed \"\n",
      "/opt/conda/envs/py37torch1.5/lib/python3.8/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "100%|██████████| 498/498 [17:48<00:00,  2.15s/it]\n",
      "100%|██████████| 52/52 [01:34<00:00,  1.81s/it]\n",
      "../utils/metrics.py:32: RuntimeWarning: invalid value encountered in true_divide\n",
      "  acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
      "100%|██████████| 40/40 [01:06<00:00,  1.66s/it]\n",
      "100%|██████████| 498/498 [17:41<00:00,  2.13s/it]\n",
      "100%|██████████| 52/52 [01:33<00:00,  1.80s/it]\n",
      "100%|██████████| 40/40 [01:06<00:00,  1.67s/it]\n",
      "100%|█████████▉| 496/498 [17:41<00:04,  2.18s/it]requests_with_retry encountered retryable exception: 500 Server Error: Internal Server Error for url: https://api.wandb.ai/files/yonyeoseok/refinenet-pytorch/108n8mgj/file_stream. args: ('https://api.wandb.ai/files/yonyeoseok/refinenet-pytorch/108n8mgj/file_stream',), kwargs: {'json': {'files': {'output.log': {'offset': 12, 'content': ['ERROR 2020-04-23T07:12:37.336240  97%|█████████▋| 482/498 [17:10<00:34,  2.14s/it]\\r']}, 'wandb-events.jsonl': {'offset': 108, 'content': ['{\"system.gpu.0.gpu\": 22.47, \"system.gpu.0.memory\": 11.8, \"system.gpu.0.memoryAllocated\": 85.47, \"system.gpu.0.temp\": 55.93, \"system.gpu.0.powerWatts\": 82.16, \"system.gpu.0.powerPercent\": 32.86, \"system.gpu.1.gpu\": 0.0, \"system.gpu.1.memory\": 0.0, \"system.gpu.1.memoryAllocated\": 6.62, \"system.gpu.1.temp\": 36.8, \"system.gpu.1.powerWatts\": 17.49, \"system.gpu.1.powerPercent\": 7.0, \"system.cpu\": 9.11, \"system.memory\": 5.92, \"system.disk\": 3.9, \"system.proc.memory.availableMB\": 89643.74, \"system.proc.memory.rssMB\": 3853.65, \"system.proc.memory.percent\": 4.04, \"system.proc.cpu.threads\": 65.0, \"system.network.sent\": 1779141968, \"system.network.recv\": 59377474, \"_wandb\": true, \"_timestamp\": 1587625947, \"_runtime\": 3648}\\n']}, 'wandb-history.jsonl': {'offset': 1488, 'content': ['{\"Train_Batch_Loss\": 0.5459315776824951, \"_runtime\": 3624.7187366485596, \"_timestamp\": 1587625927.3222342, \"_step\": 1488}\\n', '{\"Train_Batch_Loss\": 0.5299851298332214, \"_runtime\": 3626.8578374385834, \"_timestamp\": 1587625929.461335, \"_step\": 1489}\\n', '{\"Train_Batch_Loss\": 0.3705374598503113, \"_runtime\": 3628.983501434326, \"_timestamp\": 1587625931.586999, \"_step\": 1490}\\n', '{\"Train_Batch_Loss\": 0.40573054552078247, \"_runtime\": 3631.110960006714, \"_timestamp\": 1587625933.7144575, \"_step\": 1491}\\n', '{\"Train_Batch_Loss\": 0.718308687210083, \"_runtime\": 3633.225819826126, \"_timestamp\": 1587625935.8293173, \"_step\": 1492}\\n', '{\"Train_Batch_Loss\": 0.6615826487541199, \"_runtime\": 3635.354366540909, \"_timestamp\": 1587625937.957864, \"_step\": 1493}\\n', '{\"Train_Batch_Loss\": 0.5563405156135559, \"_runtime\": 3637.4963686466217, \"_timestamp\": 1587625940.0998662, \"_step\": 1494}\\n', '{\"Train_Batch_Loss\": 0.7866300940513611, \"_runtime\": 3639.627552509308, \"_timestamp\": 1587625942.23105, \"_step\": 1495}\\n', '{\"Train_Batch_Loss\": 0.6949222683906555, \"_runtime\": 3641.7782349586487, \"_timestamp\": 1587625944.3817325, \"_step\": 1496}\\n', '{\"Train_Batch_Loss\": 0.6638391613960266, \"_runtime\": 3643.9515719413757, \"_timestamp\": 1587625946.5550694, \"_step\": 1497}\\n', '{\"Train_Batch_Loss\": 0.7690202593803406, \"_runtime\": 3646.0908110141754, \"_timestamp\": 1587625948.6943085, \"_step\": 1498}\\n', '{\"Train_Batch_Loss\": 0.9548601508140564, \"_runtime\": 3648.2322533130646, \"_timestamp\": 1587625950.8357508, \"_step\": 1499}\\n', '{\"Train_Batch_Loss\": 0.5950350761413574, \"_runtime\": 3650.3933494091034, \"_timestamp\": 1587625952.996847, \"_step\": 1500}\\n', '{\"Train_Batch_Loss\": 0.515048623085022, \"_runtime\": 3652.539826631546, \"_timestamp\": 1587625955.1433241, \"_step\": 1501}\\n']}, 'wandb-summary.json': {'offset': 0, 'content': ['{\"Valid_Mean IoU :\": 0.4473427178711356, \"Valid_IoU\": [0.8622223906708831, 0.69448101835757, 0.8088387812515707, 0.34337079076270505, 0.19309536207673839, 0.2396292502733858, 0.15608255682224104, 0.4311079291211741, 0.8312200403743185, 0.2696217842122345, 0.8338498895963437, 0.5738087894566648, 0.4399697205774277, 0.8474653802786452, 0.44125735993431947, 0.43512516317875716, 0.0, 0.014875433678969756, 0.5268790895079509, 0.003953627290811999], \"Train_Batch_Loss\": 0.515048623085022, \"_timestamp\": 1587625955.1433241, \"Valid_Overall Acc:\": 0.8483081780947171, \"Test_Mean IoU :\": 0.11130114064061492, \"Test_IoU\": [0.4376495617757883, 0.0737792950498997, 0.23008985454058797, 0.10799356152137542, 0.163337355430647, 0.08353205076388327, 0.013968268732157075, 0.2206751216699852, 0.4000071951272037, 0.07986886153716509, 0.26723571024981335, 0.001190602592144288, 0.00450343351841233, 0.1344773568314306, 0.0, 0.001241212845109676, 0.0, 0.006473370626695094, 0.0, 0.0], \"Test_Epoch_Loss\": 0.055838488042354584, \"Test_FreqW Acc :\": 0.2259785587199595, \"Valid_FreqW Acc :\": 0.744285999134981, \"Test_Mean Acc :\": 0.36774489333955235, \"Test_Overall Acc:\": 0.37714881124614197, \"Valid_Epoch_Loss\": 0.011096318252384663, \"Valid_Mean Acc :\": 0.6670926328916977, \"_runtime\": 3652.539826631546, \"_step\": 1501}\\n']}}}}\n",
      "100%|██████████| 498/498 [17:45<00:00,  2.14s/it]\n",
      "100%|██████████| 52/52 [01:34<00:00,  1.82s/it]\n",
      "100%|██████████| 40/40 [01:07<00:00,  1.68s/it]\n",
      "100%|██████████| 498/498 [17:40<00:00,  2.13s/it]\n",
      "100%|██████████| 52/52 [01:35<00:00,  1.83s/it]\n",
      "100%|██████████| 40/40 [01:07<00:00,  1.68s/it]\n",
      "100%|██████████| 498/498 [17:39<00:00,  2.13s/it]\n",
      " 81%|████████  | 42/52 [01:18<00:18,  1.87s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.metrics import runningScore\n",
    "\n",
    "epoch_loss, scores = eval_model_valid_loader(model, valid_loader, step=0, commit=False)\n",
    "epoch_loss, scores = eval_model_test_loader(model, test_loader, step=0, commit=True)\n",
    "\n",
    "for epoch in range(1, 9):\n",
    "    epoch_loss = 0\n",
    "    for i, ((clear, foggy, trans), (labelId, color)) in enumerate(tqdm(train_loader)):\n",
    "        loss = train_model(model, optimizer, criteria,\n",
    "                           foggy2data(foggy),\n",
    "                           labelId2gt(labelId))\n",
    "        epoch_loss += loss\n",
    "        wandb.log({'Train_Batch_Loss': loss}, step=(i+1) + len(train_loader)*(epoch-1))\n",
    "    torch.save(model.state_dict(), os.path.join(wandb.run.dir, 'state_dict.{:02d}.pth').format(epoch))\n",
    "\n",
    "    epoch_loss, scores = eval_model_valid_loader(model, valid_loader, step=len(train_loader)*epoch, commit=False)\n",
    "    epoch_loss, scores = eval_model_test_loader(model, test_loader, step=len(train_loader)*epoch, commit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        epoch_loss = 0\n",
    "        for i, ((clear, foggy, trans), (labelId, color)) in enumerate(tqdm(valid_loader)):\n",
    "            logit = model(F.interpolate(foggy, None, 0.7, mode='bilinear')-128).cpu()\n",
    "            gt = np.vectorize(lambda x: id2label[x].train_id)(labelId.numpy())\n",
    "            gt = np.vectorize(lambda x: 19 if x == 255 else x)(gt)\n",
    "            gt = torch.from_numpy(gt)\n",
    "            scaled_logit = F.interpolate(logit, gt.shape[-2:], mode='bilinear')\n",
    "            loss = criteria(scaled_logit, gt)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "                        \n",
    "            foggy_cityscapes_val_running_metrics.update(gt.numpy(), scaled_logit.argmax(1).numpy())\n",
    "#             break\n",
    "        wandb.log({'Valid_Epoch_Loss': epoch_loss / len(valid_loader)})\n",
    "        \n",
    "        scores = foggy_cityscapes_val_running_metrics.get_scores()\n",
    "        for k, v in scores[0].items():\n",
    "            wandb.log({'Valid_{}'.format(k): v})\n",
    "        wandb.log({'Valid_IoU': list(scores[1].values())})\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        for i, (foggy, (labelId, color)) in enumerate(tqdm(test_loader)):\n",
    "            logit = model(F.interpolate(foggy, None, 0.7, mode='bilinear')-128).cpu()\n",
    "            gt = np.vectorize(lambda x: id2label[x].train_id)(labelId.numpy())\n",
    "            gt = np.vectorize(lambda x: 19 if x == 255 else x)(gt)\n",
    "            gt = torch.from_numpy(gt)\n",
    "            scaled_logit = F.interpolate(logit, gt.shape[-2:], mode='bilinear')\n",
    "            loss = criteria(scaled_logit, gt)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            \n",
    "            foggy_zurich_testv2_running_metrics.update(gt.numpy(), scaled_logit.argmax(1).numpy())\n",
    "#             break\n",
    "        wandb.log({'Test_Epoch_Loss': epoch_loss / len(test_loader)})\n",
    "        \n",
    "        scores = foggy_zurich_testv2_running_metrics.get_scores()\n",
    "        for k, v in scores[0].items():\n",
    "            wandb.log({'Test_{}'.format(k): v})\n",
    "        wandb.log({'Test_IoU': list(scores[1].values())})\n",
    "\n",
    "#     break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37torch1.5",
   "language": "python",
   "name": "py37torch1.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
